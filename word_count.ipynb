{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  count\n",
      "0         data    100\n",
      "1       market     79\n",
      "2      trading     79\n",
      "3     strategy     58\n",
      "4       factor     54\n",
      "5        order     53\n",
      "6         risk     49\n",
      "7        asset     46\n",
      "8      chapter     43\n",
      "9        price     39\n",
      "10        fund     37\n",
      "11      return     37\n",
      "12          ml     34\n",
      "13  investment     31\n",
      "14        also     29\n",
      "15         use     28\n",
      "16          ha     27\n",
      "17       trade     27\n",
      "18      source     26\n",
      "19       model     25\n",
      "         word  count\n",
      "0        data     34\n",
      "1        work     23\n",
      "2     process     11\n",
      "3          wa     11\n",
      "4        need     10\n",
      "5        team      9\n",
      "6   analytics      8\n",
      "7    decision      7\n",
      "8      better      6\n",
      "9     product      6\n",
      "10      still      6\n",
      "11        new      5\n",
      "12       last      5\n",
      "13  alignment      5\n",
      "14  objective      5\n",
      "15       even      5\n",
      "16        way      5\n",
      "17      speed      4\n",
      "18     decade      4\n",
      "19       weve      4\n",
      "            word  count\n",
      "0           data    314\n",
      "1            big    149\n",
      "2      analytics     83\n",
      "3       decision     44\n",
      "4           well     39\n",
      "5       analysis     32\n",
      "6           tool     26\n",
      "7        storage     25\n",
      "8     management     23\n",
      "9    information     22\n",
      "10     different     22\n",
      "11        making     22\n",
      "12          used     22\n",
      "13         order     21\n",
      "14        social     21\n",
      "15  organization     21\n",
      "16      customer     20\n",
      "17    processing     20\n",
      "18          need     19\n",
      "19        method     18\n",
      "              word  count\n",
      "0        analytics     86\n",
      "1          process     84\n",
      "2         business     50\n",
      "3            first     39\n",
      "4          insight     23\n",
      "5             data     22\n",
      "6     organization     21\n",
      "7   transformation     17\n",
      "8              use     17\n",
      "9         approach     17\n",
      "10             sap     17\n",
      "11           shana     17\n",
      "12        endtoend     16\n",
      "13        platform     15\n",
      "14             kpi     15\n",
      "15    intelligence     14\n",
      "16            time     14\n",
      "17          future     13\n",
      "18           based     12\n",
      "19     intelligent     11\n",
      "            word  count\n",
      "0           data    370\n",
      "1      analytics    177\n",
      "2            big    151\n",
      "3        process    112\n",
      "4       business     60\n",
      "5       decision     54\n",
      "6          first     45\n",
      "7   organization     43\n",
      "8           well     42\n",
      "9       analysis     38\n",
      "10          tool     33\n",
      "11       insight     32\n",
      "12         order     32\n",
      "13    management     31\n",
      "14          need     30\n",
      "15          work     29\n",
      "16           new     29\n",
      "17   information     28\n",
      "18          time     27\n",
      "19       provide     26\n"
     ]
    }
   ],
   "source": [
    "# Most commonly used words in all .txt files in Data directory. Saves csv results in output folder\n",
    "import sys, os\n",
    "import glob\n",
    "cwd = os.getcwd()\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters, stopwords\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "number_of_records = int(input('input number of items'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data_path = r\"data/\"\n",
    "extension = '*.txt'\n",
    "file_names = []\n",
    "\n",
    "os.chdir(data_path)\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    file_names.append(file)\n",
    "\n",
    "os.chdir(cwd)\n",
    "\n",
    "\n",
    "#Read every word of txt file (copy/paste article into blank txt file)\n",
    "def generate_word_counts(file_name):\n",
    "    with open(f'Data/{file_name}','r') as f:\n",
    "        listl=[]\n",
    "        for line in f:\n",
    "            strip_lines=line.strip()\n",
    "            listli=strip_lines.split()\n",
    "            m=listl.append(listli)\n",
    "\n",
    "    flat_list = [item for sublist in listl for item in sublist]\n",
    "    unique_string=(\" \").join(flat_list)\n",
    "\n",
    "    # Instantiate the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Define preprocess function\n",
    "    def process_text(doc):\n",
    "        sw = set(stopwords.words('english'))\n",
    "        regex = re.compile(\"[^a-zA-Z ]\")\n",
    "        re_clean = regex.sub('', doc)\n",
    "        words = word_tokenize(re_clean)\n",
    "        lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "        output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "        return output\n",
    "\n",
    "    # Define the counter function\n",
    "    def word_counter(corpus): \n",
    "        # Combine all articles in corpus into one large string\n",
    "        big_string = ' '.join(corpus)\n",
    "        processed = process_text(big_string)\n",
    "        top_10 = dict(Counter(processed).most_common(number_of_records))\n",
    "        return pd.DataFrame(list(top_10.items()), columns=['word', 'count'])\n",
    "\n",
    "    results = word_counter(flat_list)\n",
    "    print(results)\n",
    "    results.to_csv(f'output/{file_name}_word-count.csv')\n",
    "    return results\n",
    "\n",
    "\n",
    "for name in file_names:\n",
    "    generate_word_counts(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algobook.txt',\n",
       " 'article.txt',\n",
       " 'big_data.txt',\n",
       " 'deloitte.txt',\n",
       " 'whitepapers.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6d703e2c22e6bb4fd01c76b48a94d2561148bc70d401004e45f8a7fd61b65b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
