Market and Fundamental Data
- Sources and Techniques
Data has always been an essential driver of trading, and traders have long made efforts to gain an advantage from access to superior information. These efforts date back at least to the rumors that the House of Rothschild benefited handsomely from bond purchases upon advance news about the British victory at Waterloo, which was carried by pigeons across the channel.
Today, investments in faster data access take the shape of the Go West consortium of leading high-frequency trading (HFT) firms that connects the Chicago Mercantile Exchange (CME) with Tokyo. The round-trip latency between the CME and the BATS (Better Alternative Trading System) exchanges in New York has dropped to close to the theoretical limit of eight milliseconds as traders compete to exploit arbitrage opportunities. At the same time, regulators and exchanges have started to introduce speed bumps that slow down trading to limit the adverse effects on competition of uneven access to information.
Traditionally, investors mostly relied on publicly available market and fundamental data. Efforts to create or acquire private datasets, for example, through proprietary surveys, were limited. Conventional strategies focus on equity fundamentals and build financial models on reported financials, possibly combined with industry or macro data to project earnings per share and stock prices. Alternatively, they leverage technical analysis to extract signals from market data using indicators computed from price and volume information.
Machine learning (ML) algorithms promise to exploit market and fundamental data more effciently than human-defined rules and heuristics, particularly when combined with alternative data, which is the topic of the next chapter. We will illustrate how to apply ML algorithms ranging from linear models to recurrent neural networks (RNNs) to market and fundamental data and generate tradeable signals.
This chapter introduces market and fundamental data sources and explains how they reflect the environment in which they are created. The details of the trading environment matter not only for the proper interpretation of market data but also for the design and execution of your strategy and the implementation of realistic backtesting simulations.
 
We also illustrate how to access and work with trading and financial statement data from various sources using Python.
In particular, this chapter will cover the following topics:
•	How market data reflects the structure of the trading environment
 • Working with trade and quote data at minute frequency
•	Reconstructing an order book from tick data using Nasdaq ITCH
•	Summarizing tick data using various types of bars
•	Working with eXtensible Business Reporting Language (XBRL)-encoded electronic filings
•	Parsing and combining market and fundamental data to create a price-to-earnings (P/E) series
•	How to access various market and fundamental data sources using Python
You can find the code samples for this chapter and links to additional resources in the corresponding directory of the GitHub repository. The notebooks include color versions of the images.
Market data reflects its environment
Market data is the product of how traders place orders for a financial instrument directly or through interrnediaries on one of the numerous marketplaces, how they are processed, and how prices are set by matching demand and supply. As a result, the data reflects the institutional environment of trading venues, including the rules and regulations that govern orders, trade execution, and price formation. See Harris (2003)  for a global overview and Jones (2018) for details on the U.S. market.
Algorithmic traders use algorithms, including ML, to analyze the flow of buy and sell orders and the resulting volume and price statistics to extract trade signals that capture  insights into, for example, demand-supply dynamics or the behavior of certain market participants.
We will first review institutional features that impact the simulation of a trading strategy during a backtest before we start working with actual tick data created by one  such environment, namely Nasdaq.
Market microstructure — the nuts and bolts
Market microstructure studies how the institutional environment affects the trading process and shapes outcomes like price discovery, bid-ask spreads and quotes, intraday trading behavior, and transaction costs (Madhavan 2000; 2002). It is one of the fastestgrowing fields of financial research, propelled by the rapid development of algorithmic and electronic trading.
Today, hedge funds sponsor in-house analysts to track the rapidly evolving, complex details and ensure execution at the best possible market prices and design strategies that exploit market frictions. We will provide only a brief overview of these key concepts before we dive into the data generated by trading. The references contain several sources that treat this subject in great detail.
How to trade — different types of orders
Traders can place various types of buy or sell orders. Some orders guarantee immediate execution, while others may state a price threshold or other conditions that trigger execution. Orders are typically valid for the same trading day unless specified otherwise.
A market order is intended for immediate execution of the order upon arrival at the trading venue, at the price that prevails at that moment. In contrast, a limit order only executes if the market price is higher than the limit for a sell limit order, or lower than the limit for a buy limit order. A stop order, in turn, only becomes active when the  market price rises above a specified price for a buy stop order, or falls below a specified price for a sell order. A buy stop order can be used to limit the losses of short sales. Stop orders may also have limits.
 Nurnerous other conditions can be attached to orders. For example, all or none orders prevent partial execution; they are filled only if a specified number of shares is available and can be valid for a day or longer. They require special handling and are not visible to market participants. Fill or kill orders also prevent partial execution but cancel if not executed immediately. Immediate or cancel orders immediately buy or sell the number of shares that are available and cancel the remainder. Not-held orders allow the broker to decide on the time and price of execution. Finally, the market on open/close orders executes on or near the opening or closing ofthe market. Partial executions are allowed.
Where to trade — from exchanges to dark pools
Securities trade in highly organized and regulated exchanges or with varying degrees of formality in over-the-counter (OTC) markets. An exchange is a central marketplace where buyers and sellers compete for the lowest ask and highest bid, respectively.  Exchange regulations typically impose listing and reporting requirements to create transparency and attract more traders and liquidity. OTC markets, such as the Best
Market (OTCQX) or the Venture Market (OTCQB), often have lower regulatory barriers.  As a result, they are suitable for a far broader range of securities, including bonds or American Depositary Receipts (ADRs; equity listed on a foreign exchange, for example, for Nestlé, S.A.).
Exchanges may rely on bilateral trading or centralized order-driven systems that match all buy and sell orders according to certain rules. Many exchanges use intermediaries  that provide liquidity by making markets in certain securities. These intermediaries include dealers that act as principals on their own behalf and brokers that trade as agents on behalf of others. Price formation may occur through auctions, such as in the New York Stock Exchange (NYSE), where the highest bid and lowest offer are matched,  or through dealers who buy from sellers and sell to buyers.
Back in the day, companies either registered and traded mostly on the NYSE, or they traded on OTC markets like Nasdaq. On the NYSE, a sole specialist intermediated trades of a given security. The specialist received buy and sell orders via a broker and tracked limit orders in a central order book. Limit orders were executed with a priority based on price and time. Buy market orders routed to the specialist transacted with the lowest ask (and sell market orders routed to the specialist transacted with the highest bid) in the limit order book, prioritizing earlier limit orders in the case of ties. Access to all orders in the central order book allowed the specialist to publish the best bid, ask prices, and set market prices based on the overall buy-sell imbalance.
On Nasdaq, multiple market makers facilitated stock trades. Each dealer provided  their best bid and ask price to a central quotation system and stood ready to transact the specified number of shares at the specified prices. Traders would route their orders  to the market maker with the best quote via their broker. The competition for orders made execution at fair prices very likely. Market makers ensured a fair and orderly  market, provided liquidity, and disseminated prices like specialists but only had access to the orders routed to them as opposed to market-wide supply and demand. This fragmentation could create diffculties in identifying fair value market prices.
Today, trading has fragmented; instead of two principal venues in the US, there are more than thirteen displayed trading venues, including exchanges and (unregulated) alternative trading systems (ATSs) such as electronic communication networks (ECNs). Each reports trades to the consolidated tape, but at different latencies. To make matters more diffcult, the rules of engagement for each venue differ with several different pricing and queuing models.
The following table lists some of the larger global exchanges and the trading volumes for the 12 months ending 03/2018 in various asset classes, including derivatives. Typically, a minority of financial instruments account for most trading:
Exchange	Stocks				
	Market cap (USD mn)	# Listed companies	Volume / day (USD mn)	# Shares / day ('000)	# Options / day ('000)
NYSE	23,138,626	2,294	78,410	6,122	1,546
Nasdaq — US	10,375,718	2,968	65,026	7,131	2,609
 
The ATSs mentioned previously include dozens of dark pools that allow traders to execute anonymously. They are estimated to account for 40 percent of all U.S. stock trades in 2017 , compared with an estimated 16 percent in 2010. Dark pools emerged in the 1980s when the SEC allowed brokers to match buyers and sellers of big blocks of shares. The rise of high-frequency electronic trading and the 2007 SEC Order Protection rule that intended to spur competition and cut transaction costs through transparency as part of Regulation National Market System (Reg NMS) drove the growth of dark pools, as traders aimed to avoid the visibility oflarge trades (Mamudi 2017). Reg NMS also established the National Best Bid and Offer (NBBO) mandate for brokers to route orders to venues that offer the best price.
Some ATSs are called dark pools because they do not broadcast pre-trade data, including the presence, price, and amount ofbuy and sell orders as traditional exchanges are required to do. However, dark pools report information about trades to the Financial
 
Part 1 — Data, alpha factors, and portfolios
The first part covers fundamental aspects relevant across trading strategies that leverage machine learning. It focuses on the data that drives the ML algorithms and strategies discussed in this book, outlines how you can engineer features that capture the data's signal content, and explains how to optimize and evaluate the performance of a portfolio.
Chapter 1 , Machine Learningfor Trading— From Idea to Execution, summarizes how and  why ML became important for trading, describes the investment process, and outlines how ML can add value.
Chapter 2, Market and Fundamental Data — Sources and Techniques, covers how to source and work with market data, including exchange-provided tick data, and reported financials. It also demonstrates access to numerous open source data providers that we will rely on throughout this book.
Chapter 3, Alternative Datafor Finance — Categories and Use Cases, explains categories and criteria to assess the exploding number of sources and providers. It also demonstrates how to create alternative datasets by scraping websites, for example, to collect earnings call transcripts for use with natural language processing (NLP) and sentiment analysis, which we cover in the second part of the book.
Chapter 4, Financial Feature Engineering— How to Research Alpha Factors, presents the process of creating and evaluating data transformations that capture the predictive signal and shows how to measure factor performance. It also summarizes insights from research into risk factors that aim to explain alpha in financial markets otherwise deemed to be effcient. Furthermore, it demonstrates how to engineer alpha factors using Python libraries offline and introduces the Zipline and Alphalens libraries to backtest factors and evaluate their predictive power.
Chapter 5, Portfolio Optimization and Performance Evaluation, introduces how to  manage, optimize, and evaluate a portfolio resulting from the execution of a strategy. It presents risk metrics and shows how to apply them using the Zipline and pyfolio libraries. It also introduces methods to optimize a strategy from a portfolio risk perspective.
Part 2 — ML for trading — Fundamentals
The second part illustrates how fundamental supervised and unsupervised learning algorithms can inform trading strategies in the context of an end-to-end workflow.
Chapter 6, The Machine Learning Process, sets the stage by outlining how to formulate, train, tune, and evaluate the predictive performance of ML models in a systematic way. It also addresses domain-specific concerns, such as using cross-validation with financial time series to select among alternative ML models.
 
Chapter 7, Linear Models —From Risk Factors to Return Forecasts, shows how to use linear and logistic regression for inference and prediction and how to use regularization to  manage the risk of overfitting. It demonstrates how to predict US equity returns or the direction of their future movements and how to evaluate the signal content of these predictions using Alphalens.
Chapter 8, The ML4T Workflow — From Model to Strategy Backtesting, integrates the various building blocks of the ML4T workflow thus far discussed separately. It presents an end-to-end perspective on the process of designing, simulating, and evaluating  a trading strategy driven by an ML algorithm. To this end, it demonstrates how to backtest an ML-driven strategy in a historical market context using the Python libraries backtrader and Zipline.
 Chapter 9, Time-series Modelsfor Volatility Forecasts and Statistical Arbitrage, covers univariate and multivariate time series diagnostics and models, including vector  autoregressive models as well as ARCH/GARCH models for volatility forecasts. It also introduces cointegration and shows how to use it for a pairs trading strategy using a diverse set of exchange-traded funds (ETFs).
 Chapter 10, Bayesian ML — Dynamic Sharpe Ratios and Pairs Trading, presents probabilistic models and how Markov chain Monte Carlo (MCMC) sampling and variational Bayes facilitate approximate inference. It also illustrates how to use PyMC3 for probabilistic programming to gain deeper insights into parameter and model uncertainty, for example, when evaluating portfolio performance.
 Chapter 11 , Random Forests — A Long-Short StrategyforJapanese Stocks, shows how  to build, train, and tune nonlinear tree-based models for insight and prediction. It introduces tree-based ensembles and shows how random forests use bootstrap aggregation to overcome some of the weaknesses of decision trees. We then proceed to develop and backtest a long-short strategy for Japanese equities.
 Chapter 12, Boosting Your Trading Strategy, introduces gradient boosting and demonstrates how to use the libraries XGBoost, LightBGM, and CatBoost for high-performance training and prediction. It reviews how to tune the numerous hyperparameters and interpret the model using SHapley Additive explanation (SHAP) values before building and evaluating a strategy that trades US equities based on  LightGBM return forecasts.
Chapter 13, Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning, shows how to use dimensionality reduction and clustering for algorithmic trading. It uses principal and independent component analysis to extract data-driven risk factors and generate eigenportfolios. It presents several clustering techniques and demonstrates the use of hierarchical clustering for asset allocation.
Part 3 — Natural language processing
Part 3 focuses on text data and introduces state-of-the-art unsupervised learning techniques to extract high-quality signals from this key source of alternative data.
Chapter 14, Text Datafor Trading— Sentiment Analysis, demonstrates how to convert text data into a numerical format and applies the classification algorithms from Part 2 for sentiment analysis to large datasets.
Chapter 15, Topic Modeling— Summarizing Financial News, uses unsupervised learning to extract topics that summarize a large number of documents and offer more effective ways to explore text data or use topics as features for a classification model.
It demonstrates how to apply this technique to earnings call transcripts sourced in Chapter 3 and to annual reports filed with the Securities and Exchange Commission (SEC).
Chapter 16, Word Embeddingsfor Earnings Calls and SEC Filings, uses neural networks to learn state-of-the-art language features in the form of word vectors that capture semantic context much better than traditional text features and represent a very promising avenue for extracting trading signals from text data.
Part 4 — Deep and reinforcement learning
Part 4 introduces deep learning and reinforcement learning.
Chapter 1 7, Deep Learningfor Trading, introduces TensorFlow 2 and PyTorch, the most popular deep learning frameworks, which we will use throughout Part 4. It presents techniques for training and tuning, including regularization. It also builds and evaluates a trading strategy for US equities.
Chapter 18, CNNsfor Financial Time Series and Satellite Images, covers convolutional neural networks (CNNs) that are very powerful for classification tasks with unstructured data at scale. We will introduce successful architectural designs, train a CNN on satellite data (for example, to predict economic activity), and use transfer learning to speed up training. We'll also replicate a recent idea to convert financial time series into a two-dimensional image format to leverage the built-in assumptions of CNNs.
Chapter 19, RNNsfor Multivariate Time Series and Sentiment Analysis, shows how recurrent neural networks (RNNs) are useful for sequence-to-sequence modeling, including for univariate and multivariate time series to predict. It demonstrates how RNNs capture nonlinear patterns over longer periods using word embeddings  introduced in Chapter 16 to predict returns based on the sentiment expressed in SEC filings.
Chapter 20, Autoencodersfor Conditional Risk Factors and Asset Pricing, covers autoencoders for the nonlinear compression of high-dimensional data. It implements a recent paper that uses a deep autoencoder to learn both risk factor returns and factor loadings from the data while conditioning the latter on asset characteristics. We'll  create a large US equity dataset with metadata and generate predictive signals.
Chapter 21 , Generative Adversarial Networksfor Synthetic Time-series Data, presents one of the most exciting advances in deep learning. Generative adversarial networks
 
(GANs) are capable oflearning to reproduce synthetic replicas of a target data type, such as images of celebrities. In addition to images, GANs have also been applied to timeseries data. This chapter replicates a novel approach to generate synthetic stock price data that could be used to train an ML model or backtest a strategy, and also evaluate its quality.
Chapter 22, Deep Reinforcement Learning —Building a TradingAgent, presents how reinforcement learning (RL) permits the design and training of agents that learn to optimize decisions over time in response to their environment. You will see how to create a custom trading environment and build an agent that responds to market signals using OpenAI Gym.
Chapter 23, Conclusions and Next Steps, summarizes the lessons learned and outlines several steps you can take to continue learning and building your own trading strategies.
Appendix, Alpha Factor Library, lists almost 200 popular financial features, explains their rationale, and shows how to compute them. It also evaluates and compares their performance in predicting daily stock returns.
To get the most out of this book
In addition to the content summarized in the previous section, the hands-on nature of the book consists of over 160 Jupyter notebooks hosted on GitHub that demonstrate the use of ML for trading in practice on a broad range of data sources. This section describes how to use the GitHub repository, obtain the data used in the numerous examples, and set up the environment to run the code.
The GitHub repository
The book revolves around the application of ML algorithms to trading. The hands-on aspects are covered in Jupyter notebooks, hosted on GitHub, that illustrate many of the concepts and models in more detail. While the chapters aim to be self-contained, the code examples and results often take up too much space to include in their complete forms. Therefore, it is very important to view the notebooks that contain significant additional content while reading the chapter, even if you do not intend to run the code yourself.
The repository is organized so that each chapter has its own directory containing the  relevant notebooks and a README file containing separate instructions where needed, as well as references and resources specific to the chapter's content. The relevant  notebooks are identified throughout each chapter, as necessary. The repository also contains instructions on how to install the requisite libraries and obtain the data.
 
The rise of ML in the investment industry
The investment industry has evolved dramatically over the last several decades and continues to do so amid increased competition, technological advances, and a challenging economic environment. This section reviews key trends that have shaped the overall investment environment and the context for algorithmic trading and the use of ML more specifically.
The trends that have propelled algorithmic trading and ML to their current  prominence include:
•	Changes in the market microstructure, such as the spread of electronic trading and the integration of markets across asset classes and geographies
•	The development of investment strategies framed in terrns of risk-factor exposure, as opposed to asset classes
•	The revolutions in computing power, data generation and management, and statistical methods, including breakthroughs in deep learning
 
•	The outperformance of the pioneers in algorithmic trading relative to human, discretionary investors
In addition, the financial crises of 2001 and 2008 have affected how investors approach diversification and risk management. One outcome is the rise in low-cost passive investment vehicles in the form of exchange-traded funds (ETFs).
Amid low yields and low volatility following the 2008 crisis, which triggered large-scale asset purchases by leading central banks, cost-conscious investors shifted over $3.5 trillion from actively managed mutual funds into passively managed ETFs. 
Competitive pressure is also reflected in lower hedge fund fees, which dropped from the traditional 2 percent annual management fee and 20 percent take of profits to an average of 1.48 percent and 17.4 percent, respectively, in 2017.
From electronic to high-frequency trading
Electronic trading has advanced dramatically in terms of capabilities, volume, coverage of asset classes, and geographies since networks started routing prices to computer terminals in the 1960s. Equity markets have been at the forefront of this trend worldwide. See Harris (2003) and Strumeyer (2017) for comprehensive coverage of relevant changes in financial markets; we will return to this topic when we cover how to work with market and fundamental data in the next chapter.
The 199 7 order-handling rules by the SEC introduced competition to exchanges through electronic communication networks (ECNs). ECNs are automated alternative trading systems (ATS) that match buy-and-sell orders at specified prices, primarily for equities and currencies, and are registered as broker-dealers. It allows significant brokerages and individual traders in different geographic locations to trade directly without intermediaries, both on exchanges and after hours.
Dark pools are another type of private ATS that allows institutional investors to trade large orders without publicly revealing their information, contrary to how exchanges managed their order books prior to competition from ECNs. Dark pools do not publish pre-trade bids and offers, and trade prices only become public some time after execution. They have grown substantially since the mid-2000s to account for 40 percent of equities traded in the US due to concerns about adverse price movements of large orders and order front-running by high-frequency traders. They are often housed within large banks and are subject to SEC regulation.
With the rise of electronic trading, algorithms for cost-effective execution developed rapidly and adoption spread quickly from the sell-side to the buy-side and across asset classes. Automated trading emerged around 2000 as a sell-side tool aimed at  cost-effective execution that broke down orders into smaller, sequenced chunks to limit their market impact. These tools spread to the buy side and became increasingly sophisticated by taking into account, for example, transaction costs and liquidity, as well as short-terrn price and volume forecasts.
 
Direct market access (DMA) gives a trader greater control over execution by allowing them to send orders directly to the exchange using the infrastructure and market participant identification of a broker who is a member of an exchange. Sponsored access removes pre-trade risk controls by the brokers and forms the basis for high-frequency trading (HFT).
HFT refers to automated trades in financial instruments that are executed with extremely low latency in the microsecond range and where participants hold positions for very short periods. The goal is to detect and exploit ineffciencies in the market microstructure, the institutional infrastructure of trading venues.
HFT has grown substantially over the past 10 years and is estimated to make up roughly 5 5 percent of trading volume in US equity markets and about 40 percent in European equity markets. HFT has also grown in futures markets to roughly 80 percent of foreign-exchange futures volumes and two-thirds of both interest rate and Treasury 10-year futures volumes (Miller 2016).
HFT strategies aim to earn small profits per trade using passive or aggressive strategies. Passive strategies include arbitrage trading to profit from very small price differentials for the same asset, or its derivatives, traded on different venues. Aggressive strategies include order anticipation or momentum ignition. Order anticipation, also known as liquidity detection, involves algorithms that submit small exploratory orders  to detect hidden liquidity from large institutional investors and trade ahead of a large  order to benefit from subsequent price movements. Momentum ignition implies an algorithm executing and canceling a series of orders to spoof other HFT algorithms into buying (or selling) more aggressively and benefit from the resulting price changes.
Regulators have expressed concern over the potential link between certain aggressive HFT strategies and increased market fragility and volatility, such as that experienced during the May 2010 Flash Crash, the October 2014 Treasury market volatility, and the sudden crash by over 1 ,OOO points of the Dow Jones Industrial Average on August 24, 2015. At the same time, market liquidity has increased with trading volumes due to the presence of HFT, which has lowered overall transaction costs.
The combination of reduced trading volumes amid lower volatility and rising costs of technology and access to both data and trading venues has led to financial pressure. Aggregate HFT revenues from US stocks were estimated to have dropped beneath $ 1 billion in 2017 for the first time since 2008, down from $7.9 billion in 2009. This trend has led to industry consolidation, with various acquisitions by, for example, the largest listed proprietary trading firm, Virtu Financial, and shared infrastructure  investments, such as the new Go West ultra-low latency route between Chicago and Tokyo. Simultaneously, start-ups such as Alpha Trading Labs are making HFT trading infrastructure and data available to democratize HFT by crowdsourcing algorithms in return for a share of the profits.
Factor investing and smart beta funds
The return provided by an asset is a function of the uncertainty or risk associated with
 
the investment. An equity investment implies, for example, assuming a company's business risk, and a bond investment entails default risk. To the extent that specific risk characteristics predict returns, identifying and forecasting the behavior of these risk factors becomes a primary focus when designing an investment strategy. It yields valuable trading signals and is the key to superior active-management results. The industry's understanding of risk factors has evolved very substantially over time and has impacted how ML is used for trading. Chapter 4, Financial Feature Engineering —
How to Research Alpha Factors, and Chapter 5 , Portfolio Optimization and Performance Evaluation, will dive deeper into the practical applications of the concepts outlined here; see Ang (2014) for comprehensive coverage.
Modern portfolio theory (MPT) introduced the distinction between idiosyncratic and systematic sources of risk for a given asset. Idiosyncratic risk can be eliminated through diversification, but systematic risk cannot. In the early 1960s, the capital asset pricing model (CAPM) identified a single factor driving all asset returns: the return on the market portfolio in excess of T-bills. The market portfolio consisted of all tradable securities, weighted by their market value. The systematic exposure of an asset to the market is measured by beta, which is the correlation between the returns of the asset and the market portfolio.
The recognition that the risk of an asset does not depend on the asset in isolation, but rather how it moves relative to other assets and the market as a whole, was a major conceptual breakthrough. In other words, assets earn a risk premium based on their exposure to underlying, common risks experienced by all assets, not due to their specific, idiosyncratic characteristics.
Subsequently, academic research and industry experience have raised numerous critical questions regarding the CAPM prediction that an asset's risk premium depends only on its exposure to a single factor measured by the asset's beta. Instead, numerous additional risk factors have since been discovered. A factor is a quantifiable signal, attribute, or any variable that has historically correlated with future stock returns and is expected to remain correlated in the future.
These risk factors were labeled anomalies since they contradicted the emcient market hypothesis (EMH). The EMH maintains that market equilibrium would always price securities according to the CAPM so that no other factors should have predictive power (Malkiel 2003). The economic theory behind factors can be either rational, where factor risk premiums compensate for low returns during bad times, or behavioral, where agents fail to arbitrage away excess returns.
Well-known anomalies include the value, size, and mornentum effects that help predict returns while controlling for the CAPM market factor. The size effect rests on small firms systematically outperforming large firms (Banz 1981 ; Reinganum 1981 ). The value effect (Basu et. al. 1981) states that firms with low valuation metrics outperform their counterparts with the opposite characteristics. It suggests that firms with low price multiples, such as the price-to-earnings or the price-to-book ratios, perform better than their more expensive peers (as suggested by the inventors of value investing, Benjamin Graham and David Dodd, and popularized by Warren Buffet).
 
The momentum effect, discovered in the late 1980s by, among others, Clifford Asness, the founding partner of AQR, states that stocks with good momentum, in terms of recent 6-12 month returns, have higher returns going forward than poor momentum  stocks with similar market risk. Researchers also found that value and momentum factors explain returns for stocks outside the US, as well as for other asset classes, such as bonds, currencies, and commodities, and additional risk factors (Jegadeesh and Titman 1993; Asness, Moskowitz, and Pedersen 2013).
In fixed income, the value strategy is called riding the yield curve and is a form of the duration premium. In commodities, it is called the roll return, with a positive return for an upward-sloping futures curve and a negative return otherwise. In foreign exchange, the value strategy is called carry.
There is also an illiquidity premium. Securities that are more illiquid trade at low prices and have high average excess returns, relative to their more liquid counterparts. Bonds with a higher default risk tend to have higher returns on average, reflecting a credit risk premium. Since investors are willing to pay for insurance against high volatility when returns tend to crash, sellers of volatility protection in options markets tend to earn high returns.
Multifactor models define risks in broader and more diverse terms than just the market portfolio. In 1976, Stephen Ross proposed the arbitrage pricing theory, which asserted that investors are compensated for multiple systematic sources of risk that cannot be diversified away (Roll and Ross 1984). The three most important macro factors are growth, inflation, and volatility, in addition to productivity, demographic, and political risk. In 1993, Eugene Fama and Kenneth French combined the equity risk factors' size and value with a market factor into a single three-factor model that better explained cross-sectional stock returns. They later added a model that also included bond risk factors to simultaneously explain returns for both asset classes (Fama and French 1993; 2015).
A particularly attractive aspect of risk factors is their low or negative correlation. Value and momentum risk factors, for instance, are negatively correlated, reducing the risk and increasing risk-adjusted returns above and beyond the benefit implied by the risk factors. Furthermore, using leverage and long-short strategies, factor strategies can be combined into market-neutral approaches. The combination of long positions in securities exposed to positive risks with underweight or short positions in the securities exposed to negative risks allows for the collection of dynamic risk premiums.
As a result, the factors that explained returns above and beyond the CAPM were incorporated into investment styles that tilt portfolios in favor of one or more factors, and assets began to migrate into factor-based portfolios. The 2008 financial crisis underlined how asset-class labels could be highly misleading and create a false sense of diversification when investors do not look at the underlying factor risks, as asset classes came crashing down together.
Over the past several decades, quantitative factor investing has evolved from a simple approach based on two or three styles to multifactor smart or exotic beta products.
Smart beta funds have crossed $ 1 trillion AUM in 2017 , testifying to the popularity of
 
the hybrid investment strategy that combines active and passive management. Smart beta funds take a passive strategy but modify it according to one or more factors, such as cheaper stocks or screening them according to dividend payouts, to generate better returns. This growth has coincided with increasing criticism of the high fees charged by traditional active managers as well as heightened scrutiny of their performance.
The ongoing discovery and successful forecasting of risk factors that, either individually or in combination with other risk factors, significantly impact future asset returns across asset classes is a key driver of the surge in ML in the investment industry and will be a key theme throughout this book.
Algorithmic pioneers outperform humans
The track record and growth of assets under management (AUM) of firms that spearheaded algorithmic trading has played a key role in generating investor interest and subsequent industry efforts to replicate their success. Systematic funds differ from HFT in that trades may be held significantly longer while seeking to exploit arbitrage opportunities as opposed to advantages from sheer speed.
Systematic strategies that mostly or exclusively rely on algorithmic decision-making were most famously introduced by mathematician James Simons, who founded Renaissance Technologies in 1982 and built it into the premier quant firm. Its  secretive Medallion Fund, which is closed to outsiders, has earned an estimated annualized return of 3 5 percent since 1982.
D. E. Shaw, Citadel, and Two Sigma, three of the most prominent quantitative hedge  funds that use systematic strategies based on algorithms, rose to the all-time top-20 performers for the first time in 2017, in terms of total dollars earned for investors, after fees, and since inception.
D. E. Shaw, founded in 1988 and with $50 billion in AUM in 2019, joined the list at number 3. Citadel, started in 1990 by Kenneth Griffn, manages $32 billion, and ranked
5. Two Sigma, started only in 2001 by D. E. Shaw alumni John Overdeck and David Siegel, has grown from $8 billion in AUM in 2011 to $60 billion in 2019. Bridgewater, started by Ray Dalio in 1975 , had over $ 160 billion in AUM in 2019 and continues to lead due to its Pure Alpha fund, which also incorporates systematic strategies.
Similarly, on the Institutional Investors 2018 Hedge Fund 100 list, the four largest firms, and five of the top six firms, rely largely or completely on computers and trading algorithms to make investment decisions—and all of them have been growing their assets in an otherwise challenging environment. Several quantitatively focused firms climbed the ranks and, in some cases, grew their assets by double-digit percentages. Number 2-ranked Applied Quantitative Research (AQR) grew its hedge fund assets by 48 percent in 2017 and by 29 percent in 2018 to nearly $90 billion.
 
ML-driven funds attract $1 trillion in AUM
The familiar three revolutions in computing power, data availability, and statistical methods have made the adoption of systematic, data-driven strategies not only more compelling and cost-effective but a key source of competitive advantage.
As a result, algorithmic approaches are not only finding wider application in the hedge-fund industry that pioneered these strategies but across a broader range of asset managers and even passively managed vehicles such as ETFs. In particular, predictive analytics using ML and algorithmic automation play an increasingly prominent role in all steps of the investment process across asset classes, from idea generation and research to strategy formulation and portfolio construction, trade execution, and risk management.
Estimates of industry size vary because there is no objective definition of a quantitative or algorithmic fund. Many traditional hedge funds or even mutual funds and ETFs are introducing computer-driven strategies or integrating them into a discretionary environment in a human-plus-machine approach.
According to the Economist, in 2016, systematic funds became the largest driver of institutional trading in the US stock market (ignoring HFT, which mainly acts as a middleman). In 2019, they accounted for over 3 5 percent of institutional volume, up from just 18 percent in 2010; just 10% of trading is still due to traditional equity funds. Measured by the Russell 3000 index, the value of US stocks is around $31 trillion. The three types Of computer-managed funds—index funds, ETFs, and quant funds—run around 35 percent, whereas human managers at traditional hedge funds and other mutual funds manage just 24 percent.
The market research firm Preqin estimates that almost 1,500 hedge funds make a majority of their trades with help from computer models. Quantitative hedge funds are now responsible for 27 percent of all US stock trades by investors, up from 14 percent  in 2013. But many use data scientists—or quants—who, in turn, use machines to build large statistical models.
In recent years, however, funds have moved toward true ML, where artificially intelligent systems can analyze large amounts of data at speed and improve themselves through such analyses. Recent examples include Rebellion Research, Sentient, and Aidyia, which rely on evolutionary algorithms and deep learning to devise fully  automatic artificial intelligence (Al)-driven investment platforms.
From the core hedge fund industry, the adoption of algorithmic strategies has spread to mutual funds and even passively managed EFTs in the form of smart beta funds, and to discretionary funds in the form of quantamental approaches.
The emergence of quantamental funds
Two distinct approaches have evolved in active investment management: systematic
(or quant) and discretionary investing. Systematic approaches rely on algorithms for a
 
repeatable and data-driven approach to identify investment opportunities across many securities. In contrast, a discretionary approach involves an in-depth analysis of the fundamentals of a smaller number of securities. These two approaches are becoming more similar as fundamental managers take more data science-driven approaches.
Even fundamental traders now arm themselves with quantitative techniques, accounting for $ 5 5 billion of systematic assets, according to Barclays. Agnostic to specific companies, quantitative funds trade based on patterns and dynamics across a wide swath of securities. Such quants accounted for about 17 percent of total hedge fund assets, as data compiled by Barclays in 2018 showed.
Point72, with $ 14 billion in assets, has been shifting about half of its portfolio managers to a human-plus-machine approach. Point72 is also investing tens of millions of dollars into a group that analyzes large arnounts of alternative data and passes the  results on to traders.
Investments in strategic capabilities
Three trends have boosted the use of data in algorithmic trading strategies and may  further shift the investment industry from discretionary to quantitative styles: 
•	The exponential increase in the availability of digital data
•	The increase in computing power and data storage capacity at a lower cost 
•	The advances in statistical methods for analyzing complex datasets
Rising investments in related capabilities—technology, data, and, most importantly, skilled humans—highlight how significant algorithmic trading using ML has become  for competitive advantage, especially in light of the rising popularity of passive, indexed investment vehicles, such as ETFs, since the 2008 financial crisis.
Morgan Stanley noted that only 23 percent of its quant clients say they are not considering using or not already using ML, down from 44 percent in 2016.
Guggenheim Partners built what it calls a supercomputing cluster for $ 1 million at the Lawrence Berkeley National Laboratory in California to help crunch numbers for Guggenheim's quant investment funds. Electricity for computers costs another $ 1 million per year.
AQR is a quantitative investment group that relies on academic research to identify and systematically trade factors that have, over time, proven to beat the broader market. The firm used to eschew the purely computer-powered strategies of quant peers such as Renaissance Technologies or DE Shaw. More recently, however, AQR has begun to seek profitable patterns in markets using ML to parse through novel datasets, such as satellite pictures of shadows cast by oil wells and tankers.
The leading firm BlackRock, with over $ 5 trillion in AUM, also bets on algorithms to beat discretionary fund managers by heavily investing in SAE, a systematic trading firm it acquired during the financial crisis. Franklin Templeton bought Random Forest
 
Capital, a debt-focused, data-led investment company, for an undisclosed amount, hoping that its technology can support the wider asset manager.
ML and alternative data
Hedge funds have long looked for alpha through informational advantage and the ability to uncover new uncorrelated signals. Historically, this included things such as proprietary surveys of shoppers, or of voters ahead of elections or referendums.
Occasionally, the use of company insiders, doctors, and expert networks to expand knowledge of industry trends or companies crosses legal lines: a series of prosecutions of traders, portfolio managers, and analysts for using insider information after 2010 has shaken the industry.
In contrast, the informational advantage from exploiting conventional and alternative data sources using ML is not related to expert and industry networks or access to corporate management, but rather the ability to collect large quantities of very diverse data sources and analyze them in real time.
Conventional data includes economic statistics, trading data, or corporate reports. Alternative data is much broader and includes sources such as satellite images, credit card sales, sentiment analysis, mobile geolocation data, and website scraping, as well as the conversion of data generated in the ordinary course of business into valuable intelligence. It includes, in principle, any data source containing (potential) trading signals.
For instance, data from an insurance company on the sales of new car insurance policies  captures not only the volumes of new car sales but can be broken down into brands or geographies. Many vendors scrape websites for valuable data, ranging from app downloads and user reviews to airline and hotel bookings. Social media sites can also be scraped for hints on consumer views and trends.
Typically, the datasets are large and require storage, access, and analysis using scalable data solutions for parallel processing, such as Hadoop and Spark. There are more than 1 billion websites with more than 10 trillion individual web pages, with 500 exabytes (or 500 billion gigabytes) of data, according to Deutsche Bank. And more than 100 million  websites are added to the internet every year.
Real-time insights into a company's prospects, long before their results are released, can be gleaned from a decline in job listings on its website, the internal rating of its chief executive by employees on the recruitment site Glassdoor, or a dip in the average price of clothes on its website. Such information can be combined with satellite images of car parks and geolocation data from mobile phones that indicate how many people are visiting stores. On the other hand, strategic moves can be learned from a jump in job postings for specific functional areas or in certain geographies.
Among the most valuable sources is data that directly reveals consumer expenditures, with credit card information as a primary source. This data offers only a partial view
 
of sales trends, but it can offer vital insights when combined with other data. Point72, for instance, at some point analyzed 80 million credit card transactions every day. We will explore the various sources, their use cases, and how to evaluate them in detail in Chapter 3, Alternative Datafor Finance — Categories and Use Cases.
Investment groups have more than doubled their spending on alternative sets and data scientists in the past two years, as the asset management industry has tried to reinvigorate its fading fortunes. In December 2018, there were 3 7 5 alternative data providers listed on a 1 ternat ivedata . org (sponsored by provider Yipit).
Asset managers spent a total of $373 million on datasets and hiring new employees to parse them in 2017 , up 60 percent from 2016, and will probably spend a total of $616 million this year, according to a survey of investors by al ternativedata . org. It forecast that overall expenditures will climb to over $ 1 billion by 2020. Some estimates are even higher: Optimus, a consultancy, estimates that investors are spending about $ 5 billion per year on alternative data, and expects the industry to grow 30 percent per  year over the coming years.
As competition for valuable data sources intensifies, exclusivity arrangements are a key feature of data-source contracts, to maintain an informational advantage. At the same time, privacy concerns are mounting, and regulators have begun to start looking at the currently largely unregulated data-provider industry. 
Crowdsourcing trading algorithms
More recently, several algorithmic trading firms have begun to offer investment platforms that provide access to data and a programming environment to crowdsource risk factors that become part of an investment strategy or entire trading algorithms. Key examples include WorldQuant, Quantopian, and, most recently, Alpha Trading  Labs (launched in 2018).
WorldQuant was spun out of Millennium Management (AUM: $41 billion) in 2007 , for whom it manages around $5 billion. It employs hundreds of scientists and many  more part-time workers around the world in its alpha factory, which organizes the investment process as a quantitative assembly line. This factory claims to have produced 4 million successfully tested alpha factors for inclusion in more complex trading strategies and is aiming for 100 million. Each alpha factor is an algorithm that seeks to predict a future asset price change. Other teams then combine alpha factors into strategies and strategies into portfolios, allocate funds between portfolios, and manage risk while avoiding strategies that cannibalize each other. See the Appendix, Alpha Factor Library, for dozens of examples of quantitative factors used at WorldQuant.
 
Designing and executing an
ML-driven strategy
In this book, we demonstrate how ML fits into the overall process of designing,  executing, and evaluating a trading strategy. To this end, we'll assume that an MLbased strategy is driven by data sources that contain predictive signals for the target universe and strategy, which, after suitable preprocessing and feature engineering, permit an ML model to predict asset returns or other strategy inputs. The model predictions, in turn, translate into buy or sell orders based on human discretion or automated rules, which in turn may be manually encoded or learned by another ML algorithm in an end-to-end approach.
Figure 1.1 depicts the key steps in this workflow, which also shapes the organization of this book:
 
Part 1 introduces important skills and techniques that apply across different strategies  and ML use cases. These include the following;
•	How to source and manage important data sources
•	How to engineer informative features or alpha factors that extract signal content
•	How to manage a portfolio and track strategy performance
Moreover, Chapter 8, The ML4T Workflow — From Model to Strategy Backtesting, in Part 2, covers strategy backtesting. We will briefly outline each of these areas before turning to relevant ML use cases, which make up the bulk of the book in Parts 2, 3, and 4.
 
Sourcing and managing data
The dramatic evolution of data availability in terms of volume, variety, and velocity is a key complement to the application of ML to trading, which in turn has boosted industry spending on the acquisition of new data sources. However, the proliferating supply of data requires careful selection and management to uncover the potential value, including the following steps:
I. Identify and evaluate market, fundamental, and alternative data sources containing alpha signals that do not decay too quickly.
2.	Deploy or access a cloud-based scalable data infrastructure and analytical tools like Hadoop or Spark to facilitate fast, flexible data access. 
3.	Carefully manage and curate data to avoid look-ahead bias by adjusting it to the desired frequency on a point-in-time basis. This means that data should reflect only information available and known at the given time. ML algorithms trained on distorted historical data will almost certainly fail during live trading.
We will cover these aspects in practical detail in Chapter 2, Market and Fundamental Data — Sources and Techniques, and Chapter 3, Alternative Datafor Finance — Categories and Use Cases.
From alpha factor research to portfolio management
Alpha factors are designed to extract signals from data to predict returns for a given investment universe over the trading horizon. A typical factor takes on a single value for each asset when evaluated at a given point in time, but it may combine one or several input variables or time periods. If you are already familiar with the ML workflow (see Chapter 6, The Machine Learning Process), you may view alpha factors as domain-specific features designed for a specific strategy. Working with alpha factors entails a research phase and an execution phase as outlined in Figure 1.2:
 
Figure 1.2: The alpha factor research process
The research phase
The research phase includes the design and evaluation of alpha factors. A predictive factor captures some aspect of a systematic relationship between a data source and an important strategy input like asset returns. Optimizing the predictive power requires creative feature engineering in the form of effective data transformations.
